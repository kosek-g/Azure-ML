# This file has been autogenerated by version 1.46.1 of the Azure Automated Machine Learning SDK.


import numpy
import numpy as np
import pandas as pd
import pickle
import argparse


# For information on AzureML packages: https://docs.microsoft.com/en-us/python/api/?view=azure-ml-py
from azureml.training.tabular._diagnostics import logging_utilities


def setup_instrumentation():
    import logging
    import sys

    from azureml.core import Run
    from azureml.telemetry import INSTRUMENTATION_KEY, get_telemetry_log_handler
    from azureml.telemetry._telemetry_formatter import ExceptionFormatter

    logger = logging.getLogger("azureml.training.tabular")

    try:
        logger.setLevel(logging.INFO)

        # Add logging to STDOUT
        stdout_handler = logging.StreamHandler(sys.stdout)
        logger.addHandler(stdout_handler)

        # Add telemetry logging with formatter to strip identifying info
        telemetry_handler = get_telemetry_log_handler(
            instrumentation_key=INSTRUMENTATION_KEY, component_name="azureml.training.tabular"
        )
        telemetry_handler.setFormatter(ExceptionFormatter())
        logger.addHandler(telemetry_handler)

        # Attach run IDs to logging info for correlation if running inside AzureML
        try:
            run = Run.get_context()
            parent_run = run.parent
            return logging.LoggerAdapter(logger, extra={
                "properties": {
                    "codegen_run_id": run.id,
                    "parent_run_id": parent_run.id
                }
            })
        except Exception:
            pass
    except Exception:
        pass

    return logger


logger = setup_instrumentation()


def split_dataset(X, y, weights, split_ratio, should_stratify):
    '''
    Splits the dataset into a training and testing set.

    Splits the dataset using the given split ratio. The default ratio given is 0.25 but can be
    changed in the main function. If should_stratify is true the data will be split in a stratified
    way, meaning that each new set will have the same distribution of the target value as the
    original dataset. should_stratify is true for a classification run, false otherwise.
    '''
    from sklearn.model_selection import train_test_split

    random_state = 42
    if should_stratify:
        stratify = y
    else:
        stratify = None

    if weights is not None:
        X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
            X, y, weights, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, stratify=stratify, test_size=split_ratio, random_state=random_state
        )
        weights_train, weights_test = None, None

    return (X_train, y_train, weights_train), (X_test, y_test, weights_test)


def get_training_dataset(dataset_id):
    '''
    Loads the previously used dataset.
    
    It assumes that the script is run in an AzureML command job under the same workspace as the original experiment.
    '''
    
    from azureml.core.dataset import Dataset
    from azureml.core.run import Run
    
    logger.info("Running get_training_dataset")
    ws = Run.get_context().experiment.workspace
    dataset = Dataset.get_by_id(workspace=ws, id=dataset_id)
    return dataset.to_pandas_dataframe()


def get_validation_dataset(dataset_id):
    '''
    Loads the previously used dataset.
    
    It assumes that the script is run in an AzureML command job under the same workspace as the original experiment.
    '''
    
    from azureml.core.dataset import Dataset
    from azureml.core.run import Run
    
    logger.info("Running get_validation_dataset")
    ws = Run.get_context().experiment.workspace
    dataset = Dataset.get_by_id(workspace=ws, id=dataset_id)
    return dataset.to_pandas_dataframe()


def prepare_data(dataframe):
    '''
    Prepares data for training.
    
    Cleans the data, splits out the feature and sample weight columns and prepares the data for use in training.
    This function can vary depending on the type of dataset and the experiment task type: classification,
    regression, or time-series forecasting.
    '''
    
    from azureml.training.tabular.preprocessing import data_cleaning
    
    logger.info("Running prepare_data")
    label_column_name = 'TARGET'
    
    # extract the features, target and sample weight arrays
    y = dataframe[label_column_name].values
    X = dataframe.drop([label_column_name], axis=1)
    sample_weights = None
    X, y, sample_weights = data_cleaning._remove_nan_rows_in_X_y(X, y, sample_weights,
     is_timeseries=False, target_column=label_column_name)
    
    return X, y, sample_weights


def generate_preprocessor_config_0():
    '''
    Specifies a preprocessing step to be done after featurization in the final scikit-learn pipeline.
    
    Normally, this preprocessing step only consists of data standardization/normalization that is
    accomplished with sklearn.preprocessing. Automated ML only specifies a preprocessing step for
    non-ensemble classification and regression models.
    '''
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_0():
    from xgboost.sklearn import XGBClassifier
    
    algorithm = XGBClassifier(
        base_score=0.5,
        booster='gbtree',
        colsample_bylevel=1,
        colsample_bynode=1,
        colsample_bytree=1,
        gamma=0,
        gpu_id=-1,
        importance_type='gain',
        interaction_constraints='',
        learning_rate=0.300000012,
        max_delta_step=0,
        max_depth=6,
        min_child_weight=1,
        missing=numpy.nan,
        monotone_constraints='()',
        n_estimators=100,
        n_jobs=1,
        num_parallel_tree=1,
        objective='binary:logistic',
        random_state=0,
        reg_alpha=0,
        reg_lambda=1,
        scale_pos_weight=1,
        subsample=1,
        tree_method='auto',
        use_label_encoder=True,
        validate_parameters=1,
        verbose=-10,
        verbosity=0
    )
    
    return algorithm
    
    
def generate_preprocessor_config_1():
    from sklearn.preprocessing import MaxAbsScaler
    
    preproc = MaxAbsScaler(
        copy=True
    )
    
    return preproc
    
    
def generate_algorithm_config_1():
    from lightgbm.sklearn import LGBMClassifier
    
    algorithm = LGBMClassifier(
        boosting_type='gbdt',
        class_weight=None,
        colsample_bytree=1.0,
        importance_type='split',
        learning_rate=0.1,
        max_depth=-1,
        min_child_samples=20,
        min_child_weight=0.001,
        min_split_gain=0.0,
        n_estimators=100,
        n_jobs=1,
        num_leaves=31,
        objective=None,
        random_state=None,
        reg_alpha=0.0,
        reg_lambda=0.0,
        silent=True,
        subsample=1.0,
        subsample_for_bin=200000,
        subsample_freq=0,
        verbose=-10
    )
    
    return algorithm
    
    
def generate_preprocessor_config_2():
    from sklearn.preprocessing import Normalizer
    
    preproc = Normalizer(
        copy=True,
        norm='max'
    )
    
    return preproc
    
    
def generate_algorithm_config_2():
    from xgboost.sklearn import XGBClassifier
    
    algorithm = XGBClassifier(
        base_score=0.5,
        booster='gbtree',
        colsample_bylevel=1,
        colsample_bynode=1,
        colsample_bytree=0.8,
        eta=0.3,
        gamma=0,
        gpu_id=-1,
        importance_type='gain',
        interaction_constraints='',
        learning_rate=0.300000012,
        max_delta_step=0,
        max_depth=6,
        max_leaves=0,
        min_child_weight=1,
        missing=numpy.nan,
        monotone_constraints='()',
        n_estimators=10,
        n_jobs=1,
        num_parallel_tree=1,
        objective='reg:logistic',
        random_state=0,
        reg_alpha=0,
        reg_lambda=0.625,
        scale_pos_weight=1,
        subsample=0.8,
        tree_method='auto',
        use_label_encoder=True,
        validate_parameters=1,
        verbose=-10,
        verbosity=0
    )
    
    return algorithm
    
    
def generate_algorithm_config_meta():
    from azureml.training.tabular.models.stack_ensemble import Scorer
    from sklearn.linear_model import LogisticRegressionCV
    
    algorithm = LogisticRegressionCV(
        Cs=10,
        class_weight=None,
        cv=None,
        dual=False,
        fit_intercept=True,
        intercept_scaling=1.0,
        l1_ratios=None,
        max_iter=100,
        multi_class='auto',
        n_jobs=None,
        penalty='l2',
        random_state=None,
        refit=True,
        scoring=Scorer(
            metric='AUC_weighted'
        ),
        solver='lbfgs',
        tol=0.0001,
        verbose=0
    )
    
    return algorithm
    
    
def generate_algorithm_config():
    '''
    Specifies the actual algorithm and hyperparameters for training the model.
    
    It is the last stage of the final scikit-learn pipeline. For ensemble models, generate_preprocessor_config_N()
    (if needed) and generate_algorithm_config_N() are defined for each learner in the ensemble model,
    where N represents the placement of each learner in the ensemble model's list. For stack ensemble
    models, the meta learner generate_algorithm_config_meta() is defined.
    '''
    from azureml.training.tabular.models.stack_ensemble import StackEnsembleClassifier
    from sklearn.linear_model import LogisticRegressionCV
    from sklearn.pipeline import Pipeline
    
    meta_learner = generate_algorithm_config_meta()
    
    pipeline_0 = Pipeline(steps=[('preproc', generate_preprocessor_config_0()), ('model', generate_algorithm_config_0())])
    pipeline_1 = Pipeline(steps=[('preproc', generate_preprocessor_config_1()), ('model', generate_algorithm_config_1())])
    pipeline_2 = Pipeline(steps=[('preproc', generate_preprocessor_config_2()), ('model', generate_algorithm_config_2())])
    algorithm = StackEnsembleClassifier(
        base_learners=[
            ('model_0', pipeline_0),
            ('model_1', pipeline_1),
            ('model_2', pipeline_2),
        ],
        meta_learner=meta_learner,
        training_cv_folds=5
    )
    
    return algorithm
    
    
def build_model_pipeline():
    '''
    Defines the scikit-learn pipeline steps.
    '''
    from sklearn.pipeline import Pipeline
    
    logger.info("Running build_model_pipeline")
    pipeline = Pipeline(
        steps=[
            
            ('stackensemble', generate_algorithm_config()),
        ]
    )
    
    return pipeline


def train_model(X, y, sample_weights=None, transformer=None):
    '''
    Calls the fit() method to train the model.
    
    The return value is the model fitted/trained on the input data.
    '''
    
    logger.info("Running train_model")
    model_pipeline = build_model_pipeline()
    
    model = model_pipeline.fit(X, y)
    return model


def calculate_metrics(model, X, y, sample_weights, X_test, y_test, cv_splits=None):
    '''
    Calculates the metrics that can be used to evaluate the model's performance.
    
    Metrics calculated vary depending on the experiment type. Classification, regression and time-series
    forecasting jobs each have their own set of metrics that are calculated.'''
    
    from azureml.training.tabular.score.scoring import score_classification
    
    y_pred_probs = model.predict_proba(X_test)
    if isinstance(y_pred_probs, pd.DataFrame):
        y_pred_probs = y_pred_probs.values
    class_labels = np.unique(y)
    train_labels = model.classes_
    metrics = score_classification(
        y_test, y_pred_probs, get_metrics_names(), class_labels, train_labels, use_binary=True)
    return metrics


def get_metrics_names():
    
    metrics_names = [
        'accuracy',
        'f1_score_weighted',
        'iou_micro',
        'weighted_accuracy',
        'matthews_correlation',
        'iou_macro',
        'recall_score_macro',
        'recall_score_binary',
        'precision_score_classwise',
        'log_loss',
        'average_precision_score_classwise',
        'classification_report',
        'precision_score_macro',
        'norm_macro_recall',
        'recall_score_classwise',
        'balanced_accuracy',
        'confusion_matrix',
        'precision_score_weighted',
        'f1_score_classwise',
        'precision_score_binary',
        'average_precision_score_micro',
        'f1_score_macro',
        'iou',
        'AUC_macro',
        'precision_score_micro',
        'accuracy_table',
        'recall_score_micro',
        'recall_score_weighted',
        'iou_classwise',
        'f1_score_micro',
        'average_precision_score_binary',
        'average_precision_score_weighted',
        'AUC_micro',
        'f1_score_binary',
        'average_precision_score_macro',
        'iou_weighted',
        'AUC_binary',
        'AUC_weighted',
        'AUC_classwise',
    ]
    return metrics_names


def get_metrics_log_methods():
    
    metrics_log_methods = {
        'accuracy': 'log',
        'f1_score_weighted': 'log',
        'iou_micro': 'None',
        'weighted_accuracy': 'log',
        'matthews_correlation': 'log',
        'iou_macro': 'None',
        'recall_score_macro': 'log',
        'recall_score_binary': 'log',
        'precision_score_classwise': 'None',
        'log_loss': 'log',
        'average_precision_score_classwise': 'None',
        'classification_report': 'None',
        'precision_score_macro': 'log',
        'norm_macro_recall': 'log',
        'recall_score_classwise': 'None',
        'balanced_accuracy': 'log',
        'confusion_matrix': 'log_confusion_matrix',
        'precision_score_weighted': 'log',
        'f1_score_classwise': 'None',
        'precision_score_binary': 'log',
        'average_precision_score_micro': 'log',
        'f1_score_macro': 'log',
        'iou': 'None',
        'AUC_macro': 'log',
        'precision_score_micro': 'log',
        'accuracy_table': 'log_accuracy_table',
        'recall_score_micro': 'log',
        'recall_score_weighted': 'log',
        'iou_classwise': 'None',
        'f1_score_micro': 'log',
        'average_precision_score_binary': 'log',
        'average_precision_score_weighted': 'log',
        'AUC_micro': 'log',
        'f1_score_binary': 'log',
        'average_precision_score_macro': 'log',
        'iou_weighted': 'None',
        'AUC_binary': 'log',
        'AUC_weighted': 'log',
        'AUC_classwise': 'None',
    }
    return metrics_log_methods


def main(training_dataset_id=None, validation_dataset_id=None):
    '''
    Runs all functions defined above.
    '''
    
    from azureml.automl.core.inference import inference
    from azureml.core.run import Run
    
    import mlflow
    
    # The following code is for when running this code as part of an AzureML script run.
    run = Run.get_context()
    
    df = get_training_dataset(training_dataset_id)
    X, y, sample_weights = prepare_data(df)
    model = train_model(X, y, sample_weights)
    
    valid_df = get_validation_dataset(validation_dataset_id)
    X_valid, y_valid, sample_weights_valid = prepare_data(valid_df)
    
    metrics = calculate_metrics(model, X, y, sample_weights, X_test=X_valid, y_test=y_valid)
    metrics_log_methods = get_metrics_log_methods()
    print(metrics)
    for metric in metrics:
        if metrics_log_methods[metric] == 'None':
            logger.warning("Unsupported non-scalar metric {}. Will not log.".format(metric))
        elif metrics_log_methods[metric] == 'Skip':
            pass # Forecasting non-scalar metrics are not logged
        else:
            getattr(run, metrics_log_methods[metric])(metric, metrics[metric])
    cd = inference.get_conda_deps_as_dict(True)
    
    # Saving ML model to outputs/.
    signature = mlflow.models.signature.infer_signature(X, y)
    mlflow.sklearn.log_model(
        sk_model=model,
        artifact_path='outputs/',
        conda_env=cd,
        signature=signature,
        serialization_format=mlflow.sklearn.SERIALIZATION_FORMAT_PICKLE)
    
    run.upload_folder('outputs/', 'outputs/')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--training_dataset_id', type=str, default='ac874ec6-4279-440b-bbb8-38733ea650fe',     help='Default training dataset id is populated from the parent run')
    parser.add_argument('--validation_dataset_id',  type=str, default='5578efb8-aac0-4cc1-b413-54e67ad81972',     help='Default validation dataset id is populated from the parent run')
    args = parser.parse_args()
    
    try:
        main(args.training_dataset_id, args.validation_dataset_id)
    except Exception as e:
        logging_utilities.log_traceback(e, logger)
        raise